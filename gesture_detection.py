# Copyright (c) 2024 Ruveen Jayasinghe
# Licensed under the Apache License, Version 2.0
# -*- coding: utf-8 -*-
"""gesture_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QCctomajV1X5fHBE94XbTciOd5LZL8gv
"""

from google.colab import drive
drive.mount('/content/drive')

import os

DATASET_PATH = "/content/drive/MyDrive/gesture_dataset"


if os.path.exists(DATASET_PATH):
    print(" Dataset found!")
else:
    print(" Dataset not found!")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# image size & batch size
IMG_SIZE = (224, 224)  # Resize images to 224x224 to fit the mobileNetv2 model
BATCH_SIZE = 32

# Image data generator to automatically labels images based on folder name
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    DATASET_PATH + "/train",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="training"
)

val_generator = train_datagen.flow_from_directory(
    DATASET_PATH + "/train",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="validation"
)

# Print class labels
print("Class Labels:", train_generator.class_indices)

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Load MobileNetV2 without the top layer
base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights="imagenet")

# Freeze the base model
base_model.trainable = False

# Add custom layers on top
x = base_model.output
x = GlobalAveragePooling2D()(x)  # Pooling layer to reduce size
x = Dense(128, activation="relu")(x)
x = Dense(64, activation="relu")(x)
x = Dense(4, activation="softmax")(x)  # Corrected: connect the output layer to `x`

# Create final model
model = Model(inputs=base_model.input, outputs=x)  # Use `x` instead of `output_layer`

# Compile the model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Show model summary
model.summary()

# Train the model
EPOCHS = 10  # You can increase this later
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS
)

import matplotlib.pyplot as plt

# Plot accuracy and loss curves
plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.legend()
plt.title("Model Accuracy")

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.title("Model Loss")

plt.show()

import numpy as np
from tensorflow.keras.preprocessing import image

# Load and preprocess the image
img_path = "/content/drive/MyDrive/gesture_dataset/test/sample3.jpg"  # Change this to your test image path
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0  # Normalize
img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

# Predict the class
predictions = model.predict(img_array)
predicted_class = np.argmax(predictions)

# Map predictions to class labels
class_labels = {v: k for k, v in train_generator.class_indices.items()}  # Reverse mapping
predicted_label = class_labels[predicted_class]

print(f"Predicted Gesture: {predicted_label}")

!pip install mediapipe opencv-python

!pip install --upgrade mediapipe

import tensorflow as tf
import cv2
import mediapipe as mp

print("TensorFlow version:", tf.__version__)
print("OpenCV version:", cv2.__version__)
print("MediaPipe version:", mp.__version__)

import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.preprocessing.image import img_to_array

# Load MediaPipe Hands model
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# Open webcam
cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Convert frame to RGB (MediaPipe requires RGB input)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Detect hands in the frame
    results = hands.process(rgb_frame)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # Get bounding box coordinates
            h, w, c = frame.shape
            x_min, y_min, x_max, y_max = w, h, 0, 0
            for landmark in hand_landmarks.landmark:
                x, y = int(landmark.x * w), int(landmark.y * h)
                x_min, y_min = min(x_min, x), min(y_min, y)
                x_max, y_max = max(x_max, x), max(y_max, y)

            # Crop the hand region
            hand_img = frame[y_min:y_max, x_min:x_max]
            if hand_img.shape[0] > 0 and hand_img.shape[1] > 0:
                # Resize and preprocess for classification
                hand_img = cv2.resize(hand_img, (224, 224))
                hand_img = img_to_array(hand_img) / 255.0
                hand_img = np.expand_dims(hand_img, axis=0)

                # Predict gesture
                predictions = model.predict(hand_img)
                predicted_class = np.argmax(predictions)
                class_labels = {v: k for k, v in train_generator.class_indices.items()}  # Reverse mapping
                predicted_label = class_labels[predicted_class]

                # Draw bounding box and label
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                cv2.putText(frame, predicted_label, (x_min, y_min - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            # Draw hand landmarks
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    # Display output
    cv2.imshow("Hand Gesture Recognition", frame)

    # Press 'q' to exit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

model.save("/content/drive/MyDrive/gesture_model.h5")
